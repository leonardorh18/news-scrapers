# -*- coding: utf-8 -*-
"""politics

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1__3N3MHvv7UdbXmx7K5YCQ40v485KctJ
"""

from bs4 import BeautifulSoup
import requests 
import re
from datetime import date
import pandas as pd
today = date.today()
from google.colab import drive

drive.mount('/content/drive')

def removeTN(text):
  regex = re.compile(r'[\n\r\t]')
  text = regex.sub("", text)
  return text

def get_time(soup):
  date = 'none'
  articles = soup.find('time', {'itemprop': 'datePublished'})
  try:
    date = articles.get_text().split(" ")[0]
  except:
    print(" ")
  try:
    info = soup.find("div", {'class': 'story-meta__details'})
    date = info.time.get_text().split(" ")[0]
  except:
    print(" ")
  return date

def get_fulltext(soup):
  fulltext = ''
  try:
    text = soup.findAll("p", {'class': 'story-text__paragraph'})
    for p in text:
      fulltext += removeTN(p.get_text())
  except:
    print("erro texto")
  try:
    articles = soup.find('div', {'class': 'story-text'})
    ps = articles.find_all('p')
    for p in ps:
      fulltext += removeTN(p.get_text())
  except:
    print(" ")
  return fulltext

newsDict = {'content': [], 'date': [], 'link': []}
save = 0
salvar_a_cada = 50
done = False
for page in range(1,5000):
  print(page, end = ' ')
  if done:
    done = False
    break
  try:
    page = requests.get('https://www.politico.com/politics/'+str(page))
    soup = BeautifulSoup(page.content, 'html.parser')
    articles = soup.find_all('article', {'class': 'story-frag format-sm'})
    links = [article.h3.a.attrs['href'] for article in articles]
    for link in links:
      print(link)
      content = requests.get(link)
      contentSoup = BeautifulSoup(content.content, 'html.parser')

      date = get_time(contentSoup)
    
      if int(date.split("/")[2]) <= 2019:
        done = True
        break
      
      fullText = get_fulltext(contentSoup)

      newsDict['content'].append(fullText)
      newsDict['date'].append(date)
      newsDict['link'].append(link)

      save +=1
      if save == salvar_a_cada:
        save = 0
        df = pd.DataFrame.from_dict(newsDict)
        print("salvando... {} ".format(date))
        df.to_csv('/content/drive/My Drive/datasets/politics/save_politics.csv')
  except Exception as e:
    print(e)
    print("Erro...", end=' ')
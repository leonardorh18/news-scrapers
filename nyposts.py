# -*- coding: utf-8 -*-
"""nyposts

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mUg8c17G5YlWnHYelq2JyJYOAWGvg3lK
"""

from bs4 import BeautifulSoup
import requests 
import re
from datetime import date
import pandas as pd
today = date.today()
from google.colab import drive

drive.mount('/content/drive')

page = requests.get('https://nypost.com/news/page/4/')
soup = BeautifulSoup(page.content, 'html.parser')

articles = soup.findAll('h3', {'class': 'entry-heading'})

links = [article.a.attrs['href'] for article in articles]

links

pageContent = requests.get(links[0])
articleContent = BeautifulSoup(pageContent.content, 'html.parser')

textContent = articleContent.find('div', {'class': 'entry-content entry-content-read-more'})

text = textContent.findAll('p')

for t in text:
  print(t.get_text())

dateContent = articleContent.find('p', {'class': 'byline-date'})
treat_date(dateContent.get_text())

removeTN(dateContent.get_text())

def treat_date(date):
  fulldate = removeTN(date).split("|")[0]
  month = fulldate.split(" ")[0]
  day =  fulldate.split(" ")[1]
  day = re.sub(',', '', day)
  year = fulldate.split(" ")[2]

  monthDict = {'january': 1, 'february': 2, 'march': 3, 'april': 4, 'may': 5, 'june': 6 ,'july': 7, 'august': 8, 'september': 9, 'october': 10, 'november': 11, 'december': 12}
  if month.lower() in monthDict.keys():
    month = monthDict[month.lower()]

  return fulldate, day, month, year

def removeTN(text):
  regex = re.compile(r'[\n\r\t]')
  text = regex.sub("", text)
  return text

newsDict = {'content': [], 'date': [], 'source': [], 'categoria': [], 'day': [], 'month': [], 'year': [], 'link': []}
save = 0
salvarACada = 30
done = False
links = []
categorias  = ['news' ,'business', 'tech']
for categoria in categorias:
  print("---> CATEGORIA ",  categoria)
  for page in range(1,1001):
    print("Page --> ", page)
    if done:
      done = False
      break
    try:
      page = requests.get('https://nypost.com/'+categoria+'/page/'+str(page)+'/')
      soup = BeautifulSoup(page.content, 'html.parser')
      articles = soup.findAll('h3', {'class': 'entry-heading'})
      links = [article.a.attrs['href'] for article in articles]

      for link in links:
        pageContent = requests.get(link)
        articleContent = BeautifulSoup(pageContent.content, 'html.parser')
        textContent = articleContent.find('div', {'class': 'entry-content entry-content-read-more'})
        text = textContent.findAll('p')
        fulltext = ''
        for t in text:
          fulltext += removeTN(t.get_text())

        dateContent = articleContent.find('p', {'class': 'byline-date'})
        fulldate, day, month, year = treat_date(dateContent.get_text())

        if int(year)<= 2019:
          print('chegamos em 2019...')
          done = True
          break

        newsDict['content'].append(fulltext)
        newsDict['date'].append(fulldate)
        newsDict['year'].append(year)
        newsDict['month'].append(month)
        newsDict['day'].append(day)
        newsDict['link'].append(link)
        newsDict['categoria'].append(categoria)
        newsDict['source'].append('NyPosts')
        save+=1
        if save == salvarACada:
          save = 0
          df = pd.DataFrame.from_dict(newsDict)
          print("salvando... {} ".format(fulldate))
          df.to_csv('/content/drive/My Drive/datasets/nyposts/save_nyposts.csv')
    except Exception as e:
      print(e)
      continue


df.to_csv('/content/drive/My Drive/datasets/nyposts/save_nyposts.csv')
# -*- coding: utf-8 -*-
"""newsweek

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bILbUzKb6MyBlMvjQqAOdElIKMmSA-ce
"""

from bs4 import BeautifulSoup
import requests 
import re
from datetime import date
import pandas as pd
today = date.today()
from google.colab import drive

drive.mount('/content/drive')

hdr = {'User-Agent': 'Mozilla/5.0'}
page = requests.get('https://www.newsweek.com/news?page=2', headers = hdr )
soup = BeautifulSoup(page.content, 'html.parser')

soup



for article in articles:
  print(article.div.a.attrs['href'])

articles = soup.findAll('article')
links = ['https://www.newsweek.com'+article.div.a.attrs['href'] for article in articles]

pageContent = requests.get(link, headers = hdr)
articleContent = BeautifulSoup(pageContent.content, 'html.parser')

textBody = articleContent.find('div', {'class': 'article-body v_text paywall'})

text = textBody.findAll('p')

fulltext = ''
for t in text:
  fulltext += removeTN(t.get_text())

dateContent = articleContent.find('div', {'class': 'byline'})

date = treat_date(dateContent.time.get_text())

date

def removeTN(text):
  regex = re.compile(r'[\n\r\t]')
  text = regex.sub("", text)
  return text

def treat_date(date):
  date = re.search(r'(\d+/\d+/\d+)',date)
  date = date.group(1)
  day = date.split("/")[1]
  month = date.split("/")[0]
  year = '20'+date.split("/")[2]
  return date, day, month, year

newsDict = {'content': [], 'date': [], 'source': [], 'categoria': [], 'day': [], 'month': [], 'year': [], 'link': []}
save = 0
salvarACada = 30
done = False
links = []
hdr = {'User-Agent': 'Mozilla/5.0'}
categorias  = ['news', 'tech-science', 'world', 'health', 'politics', 'business', 'us']
for categoria in categorias:
  print("---> CATEGORIA ",  categoria)
  for page in range(1,1001):
    print("Page --> ", page)
    if done:
      done = False
      break
    try:
      page = requests.get('https://www.newsweek.com/'+categoria+'?page='+str(page), headers = hdr )
      soup = BeautifulSoup(page.content, 'html.parser')
      articles = soup.findAll('article')
      links = ['https://www.newsweek.com'+article.div.a.attrs['href'] for article in articles]

      for link in links:
        #print(link)
        try:
          pageContent = requests.get(link, headers = hdr)
          articleContent = BeautifulSoup(pageContent.content, 'html.parser')
          dateContent = articleContent.find('div', {'class': 'byline'})
          fulldate, day, month, year = treat_date(dateContent.time.get_text())
          
          if int(year)<= 2019:
            print('chegamos em 2019...')
            done = True
            break
          #print(fulldate)
          textBody = articleContent.find('div', {'class': 'article-body v_text paywall'})
          text = textBody.findAll('p')
          fulltext = ''
          for t in text:
            fulltext += removeTN(t.get_text())

          newsDict['content'].append(fulltext)
          newsDict['date'].append(fulldate)
          newsDict['year'].append(year)
          newsDict['month'].append(month)
          newsDict['day'].append(day)
          newsDict['link'].append(link)
          newsDict['categoria'].append(categoria)
          newsDict['source'].append('newsWeek')
          save+=1
          if save == salvarACada:
            save = 0
            df = pd.DataFrame.from_dict(newsDict)
            print("salvando... {} ".format(fulldate))
            df.to_csv('/content/drive/My Drive/datasets/newsweek/save_newsweek.csv')
        except Exception as e:
          print(e)
          continue
    except Exception as e:
      print(e)
      continue


df.to_csv('/content/drive/My Drive/datasets/newsweek/save_newsweek.csv')
# -*- coding: utf-8 -*-
"""npr

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Sw-Op_Ii4QlgIG8jdi1w3GponthNkSC
"""

from bs4 import BeautifulSoup
import requests 
import re
from datetime import date
import pandas as pd
today = date.today()
from google.colab import drive

drive.mount('/content/drive')

hdr = {'User-Agent': 'Mozilla/5.0'}
page = requests.get('https://www.npr.org/sections/politics/archive?start=16', headers = hdr )
soup = BeautifulSoup(page.content, 'html.parser')

titles = soup.findAll('h2', {'class': 'title'})

titles = soup.findAll('h2', {'class': 'title'})
links = [title.a.attrs['href'] for title in titles]

links[0]

pageContent = requests.get(links[0], headers = hdr )
articleContent = BeautifulSoup(pageContent.content, 'html.parser')

dateContent = articleContent.find("div", {'class': 'dateblock'})

treat_date(dateContent.span.get_text())

textBody = articleContent.find('div', {'id': 'storytext'})
texts = textBody.findAll('p')
fulltext = ''
for text in texts:
  fulltext += removeTN(text.get_text())

fulltext = ''
for text in texts:
  fulltext += removeTN(text.get_text())

def removeTN(text):
  regex = re.compile(r'[\n\r\t]')
  text = regex.sub("", text)
  return text

def treat_date(date):
  fulldate = removeTN(date).split("|")[0]
  month = fulldate.split(" ")[0]
  day =  fulldate.split(" ")[1]
  day = re.sub(',', '', day)
  year = fulldate.split(" ")[2]

  monthDict = {'january': 1, 'february': 2, 'march': 3, 'april': 4, 'may': 5, 'june': 6 ,'july': 7, 'august': 8, 'september': 9, 'october': 10, 'november': 11, 'december': 12}
  if month.lower() in monthDict.keys():
    month = monthDict[month.lower()]

  return fulldate, day, month, year

newsDict = {'content': [], 'date': [], 'source': [], 'categoria': [], 'day': [], 'month': [], 'year': [], 'link': []}
save = 0
salvarACada = 30
done = False
links = []
hdr = {'User-Agent': 'Mozilla/5.0'}
categorias  = ['politics', 'health','national', 'health', 'technology','science','business','world']
for categoria in categorias:
  print("---> CATEGORIA ",  categoria)
  for page in range(16,10000,24):
    print("Page --> ", page)
    if done:
      done = False
      break
    try:
      hdr = {'User-Agent': 'Mozilla/5.0'}
      page = requests.get('https://www.npr.org/sections/'+categoria+'/archive?start='+str(page), headers = hdr )
      soup = BeautifulSoup(page.content, 'html.parser')
      titles = soup.findAll('h2', {'class': 'title'})
      links = [title.a.attrs['href'] for title in titles]
      for link in links:
        #print(link)
        try:
          pageContent = requests.get(link, headers = hdr )
          articleContent = BeautifulSoup(pageContent.content, 'html.parser')
          dateContent = articleContent.find("div", {'class': 'dateblock'})
          fulldate, day, month, year = treat_date(dateContent.span.get_text())
          
          if int(year)<= 2019:
            print('chegamos em 2019...')
            done = True
            break
          #print(fulldate)
          textBody = articleContent.find('div', {'id': 'storytext'})
          texts = textBody.findAll('p')
          fulltext = ''
          for text in texts:
            fulltext += removeTN(text.get_text())

          newsDict['content'].append(fulltext)
          newsDict['date'].append(fulldate)
          newsDict['year'].append(year)
          newsDict['month'].append(month)
          newsDict['day'].append(day)
          newsDict['link'].append(link)
          newsDict['categoria'].append(categoria)
          newsDict['source'].append('npr')
          save+=1
          if save == salvarACada:
            save = 0
            df = pd.DataFrame.from_dict(newsDict)
            print("salvando... {} ".format(fulldate))
            df.to_csv('/content/drive/My Drive/datasets/npr/save_npr.csv')
        except Exception as e:
          print(e)
          continue
    except Exception as e:
      print(e)
      continue


df.to_csv('/content/drive/My Drive/datasets/npr/save_npr.csv')
# -*- coding: utf-8 -*-
"""fox news

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1chGn7enyr7Tl1MXl_nLQu-IzW1meh1VZ
"""

from bs4 import BeautifulSoup
import requests 
import re
from datetime import date
import pandas as pd
today = date.today()
from google.colab import drive

drive.mount('/content/drive')

page = requests.get('https://www.foxnews.com/api/article-search?isCategory=true&searchSelected=fox-news%2Fworld&size=11&offset=5').json()
#soup = BeautifulSoup(page.content, 'html.parser')

for page in range(0,100,5):
  print(page)

def removeTN(text):
  regex = re.compile(r'[\n\r\t]')
  text = regex.sub("", text)
  return text
def treat_date(date):
  date = date.split("-")
  year = date[0]
  month = date[1]
  day = date[2]
  return  year, month, day

#ano/mes/dia
newsDict = {'content': [], 'date': [], 'source': [], 'categoria': [], 'day': [], 'month': [], 'year': [], 'link': []}
save = 0
salvarACada = 30
done = False
links = []

categorias = ['world', 'health','tech', 'science', 'politics' , 'us']

for categoria in categorias:
  print(" --------- >> CATEGORIA {} << -------------".format(categoria))
  for pagenum in range(5,10000, 10):
    if done:
      done = False
      break;
    print('--- PAGENUM', pagenum)
    try:
      page = requests.get('https://www.foxnews.com/api/article-search?isCategory=true&searchSelected=fox-news%2F'+categoria+'&size=11&offset='+str(pagenum)).json()
      for p in page:
        try:
          fulldate = p['publicationDate'].split("T")[0]
          year, month, day = treat_date(fulldate)
          if int(year) <= 2019:
            print("chegamos em 2019...")
            done = True
            break
          link = 'https://www.foxnews.com'+p['url']
          #print(url)
          pageContent = requests.get(link)
          soup = BeautifulSoup(pageContent.content, 'html.parser')
          articleBody = soup.find('div', {'class':'article-body'}).findAll('p')
          fulltext = ''
          for p in articleBody:
            fulltext += removeTN(p.get_text())

          newsDict['content'].append(fulltext)
          newsDict['date'].append(fulldate)
          newsDict['year'].append(year)
          newsDict['month'].append(month)
          newsDict['day'].append(day)
          newsDict['link'].append(link)
          newsDict['categoria'].append(categoria)
          newsDict['source'].append('FoxNews')
          save+=1
          if save == salvarACada:
            save = 0
            df = pd.DataFrame.from_dict(newsDict)
            print("salvando... {} ".format(fulldate))
            df.to_csv('/content/drive/My Drive/datasets/foxnews/save_foxnews_'+categoria+'.csv')

        except Exception as e:
          print(e)
    except Exception as e:
      print(e)

print(links[0])
pageContent = requests.get(links[0])
soup = BeautifulSoup(pageContent.content, 'html.parser')

articleBody = soup.find('div', {'class':'article-body'}).findAll('p')

for p in articleBody:
  print(removeTN(p.get_text()))
# -*- coding: utf-8 -*-
"""washington post

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WpGgVNd3rSWnfJ-KZp8SLB-Y3eJ2vUI7
"""

from bs4 import BeautifulSoup
import requests 
import re
from datetime import date
import pandas as pd
today = date.today()
from google.colab import drive

drive.mount('/content/drive')

page = requests.get('https://www.washingtonpost.com/pb/api/v2/render/feature/section/story-list?content_origin=prism-query&url=prism://prism.query/site-articles-only,/politics&offset=20&limit=15').json()

soup = BeautifulSoup(page['rendering'], 'html.parser')

divs = soup.findAll('div', {'class': 'story-body col-xs-8 col-md-8'})

d = divs[0].div.div.div

link = d.attrs['data-pb-canonical-url']
link

pageContent = requests.get(link)
contentSoup = BeautifulSoup(pageContent.content, 'html.parser')

text = contentSoup.find('div', {'class': 'article-body'})
date = contentSoup.find('div', {'class': 'display-date'})

monthDict = {'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6 ,'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12}
if 'nov' in monthDict.keys():
  print("aq")

def removeTN(text):
  regex = re.compile(r'[\n\r\t]')
  text = regex.sub("", text)
  return text
def removeDC(text):
  text = text.replace('.', '')
  text = text.replace(',', '')
  return text

def treat_date(date):
  date = date.split("at")[0]
  date = date.split(" ")
  monthDict = {'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6 ,'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12}
  if date[0].lower() in monthDict.keys():
    month = monthDict[date[0].lower()]
  else:
    month = date[0].lower()
  day = date[1]
  year = date[2]
  fullDate = date[0]+' '+date[1]+' '+date[2]
  return fullDate, day, month, year

date, day, month, year = treat_date(removeDC(date.get_text()))

text.get_text()

newsDict = {'content': [], 'date': [], 'source': [], 'categoria': [], 'day': [], 'month': [], 'year': [], 'link': []}
categorias = ['politics', 'technology', 'health' , 'education', 'immigration', 'national-security', 'religion', 'science', 'investigations', 'sports', 'business', 'coronavirus']

save = 0
salvarACada = 30
done = False
links = []

for categoria in categorias:
  print("Categoria ---------> ", categoria)
  for pagenum in range(15,1000,15):
    print(" OFFSET -- ", pagenum)
    if done:
      done = False
      break
    try:
      page = requests.get('https://www.washingtonpost.com/pb/api/v2/render/feature/section/story-list?content_origin=prism-query&url=prism://prism.query/site-articles-only,/'+categoria+'&offset='+str(pagenum)+'&limit=15').json()
      soup = BeautifulSoup(page['rendering'], 'html.parser')
      divs = soup.findAll('div', {'class': 'story-body col-xs-8 col-md-8'})
      for index in range(len(divs)):
        try:
          post = divs[index].div.div.div
          link = post.attrs['data-pb-canonical-url']
          pageContent = requests.get(link)
          contentSoup = BeautifulSoup(pageContent.content, 'html.parser')
          date = contentSoup.find('div', {'class': 'display-date'})
          fulldate, day, month, year = treat_date(removeDC(date.get_text()))

          if int(year) <= 2019:
            print("Chegamos em 2019...")
            done = True
            break

          text = contentSoup.find('div', {'class': 'article-body'})
          fulltext = removeTN(text.get_text())

          newsDict['content'].append(fulltext)
          newsDict['date'].append(fulldate)
          newsDict['year'].append(year)
          newsDict['month'].append(month)
          newsDict['day'].append(day)
          newsDict['link'].append(link)
          newsDict['categoria'].append(categoria)
          newsDict['source'].append('washington-post')
          save+=1
          if save == salvarACada:
            save = 0
            df = pd.DataFrame.from_dict(newsDict)
            print("salvando... {} ".format(fulldate))
            df.to_csv('/content/drive/My Drive/datasets/washpost/save_washpost_'+categoria+'.csv')
        except Exception as e:
          print(e)
    except Exception as e:
      print(e)

df = pd.DataFrame.from_dict(newsDict)
print("salvando... {} ".format(fulldate))
df.to_csv('/content/drive/My Drive/datasets/washpost/save_washpost_'+categoria+'.csv')
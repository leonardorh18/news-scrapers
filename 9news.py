# -*- coding: utf-8 -*-
"""9news

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fPxh4BVWIOzz3HVsnFlfXsFy8wmpN5jx
"""

from bs4 import BeautifulSoup
import requests 
import re
from datetime import date
import pandas as pd
today = date.today()
from google.colab import drive

drive.mount('/content/drive')

page = requests.get('https://www.9news.com.au/coronavirus/1')
soup = BeautifulSoup(page.content, 'html.parser')

feed = soup.find('div', {'class':'feed__stories'})

story = feed.find_all('div', {'class': 'story__wrapper'})

links = [link.a.attrs['href'] for link in story]

contentPage = requests.get(links[0])
contentSoup = BeautifulSoup(contentPage.content, 'html.parser')

date = contentSoup.find('time', {'class': 'text--byline'})

date = date.get_text()

splt = date.split(" ")

splt = splt[1] +' '+ re.sub(",", '', splt[2])+' '+ splt[3]

splt

def treat_date(date):
  splt = date.split(" ")
  splt = splt[1] +' '+ re.sub(",", '', splt[2])+' '+ splt[3]
  return splt

text = contentSoup.find('div', {'class': 'article__body-croppable'})

text.get_text()

#categorias = ['crime', 'coronavirus', 'technology', 'health', 'usa', 'motoring']
#categorias = ['technology', 'health', 'usa', 'motoring']
categorias = ['usa', 'motoring']
newsDict = {'link': [], 'content': [],  'date': [],  'categoria': [], 'source': []}
salvar_a_cada = 20
save = 0
done = False
for categoria in categorias:
  for page in range(1,1000):
    print(" --- page ", page)
    if done:
      done = False
      break
    try:
      page = requests.get('https://www.9news.com.au/'+categoria+'/'+str(page))
      soup = BeautifulSoup(page.content, 'html.parser')
      feed = soup.find('div', {'class':'feed__stories'})
      story = feed.find_all('div', {'class': 'story__wrapper'})
      links = [link.a.attrs['href'] for link in story]
      for link in links:
        try:
          print(link)
          contentPage = requests.get(link)
          contentSoup = BeautifulSoup(contentPage.content, 'html.parser')
          date = contentSoup.find('time', {'class': 'text--byline'})
          date = treat_date(date.get_text())

          if int(date.split(" ")[2]) <= 2019:
            done = True
            break

          text = contentSoup.find('div', {'class': 'article__body-croppable'})
          fullText = text.get_text()

          newsDict['link'].append(link)
          newsDict['content'].append(fullText)
          newsDict['date'].append(date)
          newsDict['categoria'].append(categoria)
          newsDict['source'].append('9news')
          save+=1
          if save == salvar_a_cada:
            save = 0
            df = pd.DataFrame.from_dict(newsDict)
            print("salvando... {} ".format(date))
            df.to_csv('/content/drive/My Drive/datasets/9news/save_9news.csv')
        except Exception as e:
          print(e)

    except Exception as e:
      print(e)
# -*- coding: utf-8 -*-
"""deadline

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16oB4a1uMcqweh6FjlV7sfen4c9JGBKoq
"""

from bs4 import BeautifulSoup
import requests 
import re
from datetime import date
import pandas as pd
today = date.today()
from google.colab import drive

drive.mount('/content/drive')

def removeTN(text):
  regex = re.compile(r'[\n\r\t]')
  text = regex.sub("", text)
  return text

def treatTime(time):
  time = removeTN(time.get_text())
  time = time.split(" ")
  time = time[0:3]
  time[1] = re.sub(",", '', time[1])
  return time[0] + ' ' + time[1] + ' ' + time[2]

categorias = ['business']
done = False
newsDict = {'content': [],'date': [],'author': [] , 'title': [], 'resume': [], 'link': [], 'categoria': [] }
show = False
salvarAcada = 50
save = 0  
for categoria in categorias:
  print("Categoria ", categoria)
  for pagenum in range(1, 5000):
    print(pagenum, end= " ")
    if done == True:
      done = False
      break

    try:
      page = requests.get('https://deadline.com/v/'+categoria+'/page/'+str(pagenum)+'/')
      soup = BeautifulSoup(page.content, 'html.parser')
      mydivs = soup.findAll("div", {"class": "river-story a-archive-grid__story "}, )

      for div in range(len(mydivs)):

        resume = mydivs[div].find('div', {'class':'c-tagline c-tagline--a-content__link pmc-u-font-family-georgia pmc-u-color-grey-medium u-hidden@mobile-max pmc-u-font-size-16 pmc-u-font-size-18@desktop-xl pmc-u-line-height-copy'})
        resume = removeTN(resume.get_text())

        date = mydivs[div].find("span", {'class': 'o-info-nugget__text'})
        date = treatTime(date)

        if int(date.split(" ")[2]) <= 2019:
          print("Chegamos em 2019...")
          done = True
          break

        link = mydivs[div].find('a', {'class': 'c-title__link pmc-u-color-black u-color-brand-red:hover'}, href= True)
        link = link.attrs['href']

        article = requests.get(link)
        soup = BeautifulSoup(article.content, 'html.parser')
        articlePage = soup.find('div', {'class': 'a-content pmc-u-line-height-copy pmc-u-font-family-georgia pmc-u-font-size-16 pmc-u-font-size-18@desktop'})
        
        author = soup.find('p', {'class': 'pmc-u-margin-tb-00 pmc-u-font-size-14'})
        author = removeTN(author.get_text())
        author = re.sub("By ", '', author)

        title = soup.find('h1', {'class': 'c-title pmc-u-font-size-20 pmc-u-font-size-38@tablet pmc-u-font-size-46@desktop-xl u-text-align-center@mobile-max u-letter-spacing-0025 pmc-u-line-height-normal u-line-height-45@tablet pmc-u-padding-t-1 pmc-u-padding-t-050@mobile-max'})
        title = title.get_text()
        
        paragraph = articlePage.findAll('p')
        fullText = ''
        for p in paragraph:
          if 'blogherads' not in p.get_text():
            text = removeTN(p.get_text())
            fullText = fullText + text
        
        newsDict['content'].append(fullText)
        newsDict['date'].append(date)
        newsDict['link'].append(link)
        newsDict['author'].append(author)
        newsDict['title'].append(title)
        newsDict['resume'].append(resume)
        newsDict['categoria'].append(categoria)

        save +=1
        if save == salvarAcada:
          save = 0
          df = pd.DataFrame.from_dict(newsDict)
          print("salvando... {} ".format(date))
          df.to_csv('/content/drive/My Drive/datasets/deadline/saved_deadline.csv')
        if show:
          print("---------------")
          print(author)
          print(date)
          print(link)
          print(fullText)
          print("---------------")

    except Exception as e: 
      print(e)
      print("Erro...", end = ' ')
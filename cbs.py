# -*- coding: utf-8 -*-
"""cbs

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YYNr40VZogJW0WH7T475fSambfKcwAFV
"""

from bs4 import BeautifulSoup
import requests 
import re
from datetime import date
import pandas as pd
today = date.today()
from google.colab import drive

drive.mount('/content/drive')

def removeTN(text):
  regex = re.compile(r'[\n\r\t]')
  text = regex.sub("", text)
  return text
def get_text(soup):
  fulltext = ' '
  try:
    content = soup.find('section', {'class': 'content__body'})
    ps = content.findAll('p')
    for p in ps:
      fulltext += removeTN(p.get_text())
  except:
    print("", end = '')
  try:
    text = soup.findAll('p', {'class': False, 'id': False})
    for t in text:
      fulltext+= removeTN(t.get_text())
    
  except:
    print("", end = '')
  #print(fulltext)
  return fulltext

def get_data(soup):
  date = 'none'
  try:
    date = soup.find('p', {'class': 'content__meta content__meta--timestamp'})
    date = date.time.get_text().split("/")[0].split(',')
    date = date[0] + date[1]
    #print(date)
  except:
    print("", end = '')
  try:
    dateinfo = soup.find('span', {'class':'time'})
    date = dateinfo.get_text()
    date = date.split(",")
    date = date[0] + date[1]
    #print(date)
    
  except:
    print("", end = '' )
  return date

#categorias = ['politics']
categorias = ['politics','health', 'science', 'crime', 'world', 'technology']
categoriaDict = {'health': 15, 'politics': 10, 'science': 15, 'crime': 15, 'world': 15,  'technology': 15}
save = 0
salvarACada = 50
done = False
newsDict = {'link': [], 'content': [],  'date': [],  'categoria': []}
for categoria in categorias:
  for key in categoriaDict.keys():
    if key == categoria:
      print("CATEGORIA ----", categoria)
      aux = categoriaDict[categoria]

  for pagenum in range(1,1000):
    print("page:", pagenum, end = ' ')
    if done:
      done = False
      break
    try:
      page = requests.get('https://www.cbsnews.com/latest/'+categoria+'/'+str(pagenum)+'/')
      soup = BeautifulSoup(page.content, 'html.parser')
      articles = soup.findAll("a", {"class": "item__anchor"}, href = True)
      links = [a.attrs['href'] for a in articles]

      for link in links[0:aux]:
        if link.split("/")[3] == 'news':
          print(link)
          articlePage = requests.get(link)
          articleSoup = BeautifulSoup(articlePage.content, 'html.parser')

          date = get_data(articleSoup)
          if 'Updated' in date:
            #print("---------", date)
            date = re.sub("Updated on:  ", '', date)
  
          try:
            if int(date.split(" ")[2]) <=2019:
              done= True
              print("Chegamos em 2019...")
              break
          except:
            print("Erro ao ver se ja Ã© 2019..")
            
          fullText = get_text(articleSoup)

          newsDict['link'].append(link)
          newsDict['content'].append(fullText)
          newsDict['date'].append(date)
          newsDict['categoria'].append(categoria)

          save+=1
          if save==salvarACada:
            save = 0
            df = pd.DataFrame.from_dict(newsDict)
            print("salvando... {} ".format(date))
            df.to_csv('/content/drive/My Drive/datasets/cbs/save_cbs_'+categoria+'.csv')

    except Exception as e:
      print(e)
      print("Erro...", end = '')
      continue